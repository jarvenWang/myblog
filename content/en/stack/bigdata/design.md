---
author: "wangjinbao"
title: "数仓模型设计原则"
date: 2023-06-24
description: "明确业务需求、清晰数据架构、有效维度建模、模块化可维护、数据质量保证、保证数据安全、可操作性和易用性"
draft: false
hideToc: false
enableToc: true
enableTocContent: false
author: wangjinbao
authorEmoji: 👻
tags:
- bigdata
- categories:

---
>PS:数仓的设计流程：
> 步骤一：了解业务流程

# 建设过程：
![/images/docImages/design.png](/images/docImages/design.png)

# 一、数据整合及管理体系
## 定位与价值
建设统一的、规范化的数据接入层（ODS）和 数据中间层（DWD 和 DWS），通过 数据服务 和 数据产品
## 体系架构
包括三块：
1. 维表
2. 事实表
3. 指标

![/images/docImages/design2.png](/images/docImages/design2.png)

## 名词术语
1. 主题域（数据域）：
2. 业务过程：
3. 时间周期；
4. 修饰类型
5. 修饰词：
6. 度量：原子指标
7. 维度
8. 维度属性
9. 派生指标

## 指标体系
包括：原子指标、派生指标、修饰类型、修饰词、时间周期。

`派生指标 = 原子指标 + 时间周期 + 修饰词`
 
阿里巴巴常用的时候周期修饰词如下表：

| 中文名    | 英文名 |
|--------|--|
| 最近1天   | 1d |
| 最近3天   | 3d |
| 最近7天   | 1w |
| 最近14天  | 2w |
| 最近30天  | 1m |
| 最近60天  | 2m |
| 最近90天  | 3m |
| 最近180天 | 6m |
| 自然周    | cw |
| 自然月    | cm |
| 自然季度   | cq |
| 截至当日   | td |
| 财年     | fy |
| 最近1小时  | 1h |
| 准实时    | ts |
| 未来7天   | f1w |
| 未来4周   | f4w |

## 模型设计
### 数仓分层
数据模型的维度设计 主要以 维度建模理论 为基础，基于维度数据模型总线架构，构建一致性的 维度 和 事实。

数据模型一般分为三层：<font color="lightgreen">操作数据层（ODS）、公共维度模型层（CDM） 和 应用数据层（ADS）,其中公共维度模型层包括 明细数据层（DWD）和汇总数据层（DWS）</font>

#### 操作数据层（ODS）
定义：把操作系统数据几乎无处理地存放在数据仓库中
+ 同步：结构化数据增量或全量同步到 MaxCompute
+ 结构化：非结构（日志）结构化处理并存储到 MaxCompute
+ 累积历史、清洗：根据数据业务需求及稽核和审计要求保存历史数据、清洗数据


模型层次关系如下图：
![/images/docImages/design3.png](/images/docImages/design3.png)

#### 公共维度模型层（CDM）
存放 <font color="lightgreen">明细数组层(DWD) 、汇总数据层(DWS) 及 维度指标数据(DIM)</font>
明细数组层(DWD) 和 维度数据一般根据ODS层数据加工生成。
汇总数据层(DWS) 由维度数据和明细数据加工生成。

#### 应用数据层（ADS）
存储数据产品个性化的统计指标数据，根据CDM层与ODS层加工生成

### 模型建议
#### 基本原则：
1. 高内聚和低耦合：
2. 核心模型与扩展模型分离
3. 公共处理逻辑下沉及单一
4. 成本与性能平衡
5. 数据可回滚
6. 一致性
7. 全名清晰、可理解

### 构建流程

## 维度建模
维度建模：数据仓库的经典模型了解一下。

维度建模 以分析决策的需求出发构建模型，构建的数据模型为分析需求（也就是我们通常所说的数据分析）服务。它重点解决如何更快速完成分析需求，同时还有较好的大规模复杂查询的响应性能。

说白了，所谓的维度建模就是一种组织数据仓库的形式、模型，用这种方式组织搭建的数据仓库，对快速支持数据分析有着巨大的帮助。目前也是比较主流的数仓模型了

### 维度建模基础知识

#### （1）事实与事实表（Fact Table）
<font color='lightgreen'>事实表:</font> 是指其中保存了大量业务度量数据的表，是数仓最核心的表。
事实表中的度量值一般称为 <font color='lightgreen'>事实</font> 。通常，最有用的事实就是数字类型的事实和可加类型的事实。事实表的粒度，决定了数据仓库中数据的详细程度。

下图为例。中间的表：服装销售明细表，就是一张事实表。其中的销售金额、成本、利润，都是事实，也是我们需要分析的目标数据。
![/images/docImages/shishi.png](/images/docImages/shishi.png)

一般事实表中只存放数字或一些flag用来统计，如：销售金额、成本等。另外，通常事实表中的数据不允许修改，新的数据只是简单地添加到事实表中。
事实表特点：数据量庞大、列数少、经常变化。这个比较好理解，因为实事表是一张业务表嘛，业务肯定是不断有新的数据加进来的。

#### （2）维度与维度表（Dimension Table）
维度表是用户来分析数据的窗口，比如时间、地区、用户等。
维度表中包含事实表中记录的特性，有些特性提供描述性信息，有些特性指定如何汇总事实数据表数据，以便为分析者提供有用的信息。
![/images/docImages/shishi.png](/images/docImages/shishi.png)

例如上图，包括了五张维度表：时间维表、产品维表、地域维表、用户维表、支付维表。每一张维度表对应现实世界中的一个对象或概念。
每一张维度表利用维度关键字（图中标红字段）通过事实表中的外键约束事实表的中某一行。
维度表等特点：很多描述性的列，行数较少，内容较固定。这个也好理解，比如地域，省市区县这些内容十几年都不会有啥变化

#### （3）粒度
粒度是指数据仓库的数据单位中，保存数据的细化程度的级别。简单点来看，在实事表中一条记录所表达的业务细节，就是粒度。
![/images/docImages/lidu.png](/images/docImages/lidu.png)

通常，为了便捷的下钻分析，我们都会使用到最小粒度。比如订单表中，最小粒度就是一条订单的记录。
使用最小粒度的优点：
+ 可以频繁的ETL操作
+ 很多数据挖掘需要最小粒度数据
+ 方便向下钻取

当然，使用最小粒度也有缺点：
+ 存储和维护代价较高
+ 需要进一步构建汇总事实表来支持汇总数据查询

#### （4）切片、切块与旋转
切片 与 切块 主要是用来进行数据分析的。我们以下面的三维（产品、年度、地区）为例。
![/images/docImages/qipian.png](/images/docImages/qipian.png)
+ 切片：从多维数组中选定一个二维子集，切出一个“平面”  。比如选中上图的2011年，这就是一个切片。
+ 切块：从多维数组中选定一个三维子集，切出一个“立方体” 。比如上图中，年度选择了2011、2012，然后看所有的数据内容，这就是一个切块。
+ 旋转：改变一个报告（页面）显示的维方向

#### （5）钻取
根据维层次，改变数据分析的粒度，就是钻取分析，主要包括`上钻`（也叫上卷）和`下钻`。其实Excel中的数据透视就是各种上卷和下钻。

![/images/docImages/shangzhuan.png](/images/docImages/shangzhuan.png)
+ 下钻：从汇总数据深入到细节数据进行观察或增加新维
+ 上钻（上卷）：从某一维上将低层次的细节数据概括到高层次的汇总数据或减少维数
+ 钻透：直接下钻到最明细的数据。

### 维度建模的三种模型
上面介绍了关于维度建模的一些基础知识，下面聊一聊维度建模的几种具体模型：
<font color='lightgreen'>星型模型</font>、<font color='lightgreen'>雪花模型</font>、<font color='lightgreen'>星座模型</font>。

### （1）星型模型
所谓 `星型模型` ，具体表现是：**事实被维度所包围，且维度没有被新的表连接**。如下图:
![/images/docImages/xing.png](/images/docImages/xing.png)
每个维表都有一个维作为主键，所有这些维的主键组合成事实表的主键。
可以看出，星型模型是比较单纯的模型，像星星一样触角没有延伸了。

### （2）雪花模型
所谓的雪花模型，是有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上，就像雪花一样。如下图：

![/images/docImages/xuehua.png](/images/docImages/xuehua.png)

雪花模型去除了数据冗余，更贴近与业务。尽可能降低数据存储量以及联合较小的维表来改善查询性能。

可以想象到，雪花模型分析数据时，操作比较复杂。毕竟需要关联的内容越来越多。但数据的存储量下来了，因为冗余信息进行了提炼嘛

### （3）星座模型
无论是星型模型还是雪花模型，都是单事实表的情况。但通常来讲，实践当中大部分情况都是多事实表的。这时就是需要星座模型了。

所谓星座模型，是多个事实表共享维度表， 因而可以视为星型模型的集合，故亦称星座模型
![/images/docImages/xingzhuo.png](/images/docImages/xingzhuo.png)

星座模型是数据仓库最常使用的模型。

## 实事表和维度表
### （1）实事表
事实表主要包括以下四种：
1. 事务事实表 ：
   该类型表的一行对应空间或时间上某点的度量事件。与粒度同层次的事实表，可以直接将事实字段进行Sum、Count等聚合操作。
2. 周期快照事实表 ：
   该类型表中的每行汇总了发生在某一标准周期，如某天、某周、某月的多个度量事件。这类表非常适合跟踪长期的过程，如银行账户和其他形式的财务报表。
3. 无事实事实表 :
   没有度量事实，仅记录一系列某一时刻发生的多维实体。非事实型事实表通常用来跟踪一些时间或者说明某些活动的范围。
4. 累积快照事实表 ：
   行汇总了发生在过程开始和结束之间可预测步骤内的度量事件。管道或工作流过程（履行订单、索赔过程），都可以在此类事实表中被建模。

### （2）维度表
关于维度表，主要看看 `缓慢变化维`
比如，在一个零售业数据仓库中，事实表存着销售人员的销售记录，某天一个销售人员从北京分公司调到上海分公司了，那么如何来保存、处理这个变化呢？
如果我们要统计北京地区或上海地区的总销售情况的时候，这个销售人员的销售记录应该算在北京还是算在上海？当然是调离前的算在北京，调离后的算在上海,但是如何标记这个销售人员所属区域？这里就需要处理一下这个维度的数据，即我们缓慢变化维需要做的事情。

#### 处理缓慢变化维的三种方式：
1. 直接覆盖原值：这个比较简单粗暴。但是如果想要做历史分析的话，就比较难了，一般不太覆盖。
2. 增加属性列：增加一个新列，来记录变化。这种适合变化比较少的情况，如果经常变化，增加无限量个字段明显不合适。
3. 增加维度行：直接增加一条新纪录，并用一个专门的字段（可以是时间、版本、是否生效等等）进行标识，区分哪个数据是最新的

## 数仓的特性

### （1）面向主题的
传统数据库都是面向应用的，即功能层面需要什么数据，数据库就是什么数据。但数仓是基于分析人员分析时关注的 <font color='lightgreen'>主题</font> ，进行数据的组织。这就为数仓的便捷分析应用奠定了基础。
比如电商行业中，一般分为以下主题，供参考：

业务主题：`用户、采购、配送、流量、仓储、商品、订单、售后、促销`

### （2）集成的
集成的含义，指的是数据仓库的数据是从原有的分散的数据库数据抽取来的，进行了系统的整合。同一个主题的数据往往是 <font color='lightgreen'>多个业务模块进行的整合</font> 。同时，数仓通过分层，将数据从底层一直整合到顶层，实现统一。
### （3）相对稳定的
数据仓库的数据和业务数据库的差别之一，是数仓的数据由于为了满足各种查询分析的需求，是 <font color='lightgreen'>不能对数据进行修改的</font> 。
比如用户的账户余额，在业务库中是不断变化的，但在数仓中，是固定时间进行的同步，发生了变化是通过时间分区进行区别，而不是去修改数仓中的数据。
### （4）随时间变化的
每隔一段固定的时间间隔后，抽取运行数据库系统中产生的数据，转换后集成到数据仓库中。随着时间的变化，数据以更高的综合层次被不断综合，以适应趋势分析的要求。因此，数仓的数据是随着时间变化的。

## 数仓的意义和价值
### （1）数仓极大提升BI分析效率，促进业务发展
在没有数仓的时候，领导想要个用户购买行为分析，分析师要怎么办呢？往往得找订单研发提订单数据，找商品研发提商品数据，然后在SQL、Excel中进行各种数据处理、各种数据计算，最终才能做出购买分

但有了数仓以后，只要给数据部的同事提一下需求，甚至如果自己会的话，也可以自己完成数据的提取分析工作了。效率简直高了一大截。这在互联网快速响应、业务变化异常快的今天，更是异常重要。
### （2）提升数据开发效率，降低重复性建设成本
数仓是通过有组织的分层，将数据集中在一起。对于很多场景，数仓通过维度模型可以很灵活的进行支持，提升了业务数据研发的效率

比如之前想开发个数据，都是需要从原始数据层面开始抽取。有了数仓，可以自上而下，先看应用层是否可以满足需求，如果不满足再看通用层。这样避免了每次数据开发都是烟囱式重复开发。

### （3）可以提高数据质量和一致性
因为现在的数据都集中到统一的数仓里了，所有的指标、逻辑都维持一套统一的口径，避免了出现各个部门数据不一致的情况。公司整体的数据质量也相应提升